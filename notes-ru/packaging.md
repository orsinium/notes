# Packaging

## setuptools

Когда я только начал изучать питон, я сделал свой первый проект. И я захотел, чтобы проект стал пакетом, который можно скачать и установить с pypi.org. Что для этого нужно?

Нужно создать [вот такой файл](https://docs.python.org/3/distutils/introduction.html#a-simple-example):

```python
from distutils.core import setup
setup(name='foo',
      version='1.0',
      py_modules=['foo'],
      )
```

Я сохранил оригинальное форматирование из документации `distutils`, чтобы вы почувствовали дух питона, свободного от кодстайлов. `Distutils` состоит из плохих решений, таких как глобальные переменные и ненужное дублирование кода. Но главное плохое решение, что это не инструмент, который умеет читать метаданные и работать с ними, а скрипт, который можно только исполнить. Соответственно, эти метаданные невозможно прочитать, не исполнив скрипт, а исполнения скрипта может быть довольно болезненным: нужно создать окружение, поместить туда все файлы проекта (потому что что-то из этого может понадобиться скрипту), установить зависимости, нужные для скрипты, запустить его и понадеяться, что это не отправит куда-нибудь ssh-ключи текущего пользователя. Причём даже этого ещё недостаточно, потому что в `distutils` нет команды, чтобы получить метаданные, можно только исполнять с ними разные действия, такие как сборка пакета или установка зависимостей. Думаю, это то самое решение, которое сильно повлияло на дальнейшую судьбу пакетов в питоне.

К тому же, у `distutils` есть ещё один огромный недостаток: он в стандартной библиотеке. Казалось бы, а где же ему быть ещё, но это накладывает множество ограничений: сложно контрибьютить, релизы привязаны к релизам питона, нужно всеми силами соханять обратную совместимость. Решением стал `setuptools` -- обёртка вокруг `distutils`, добавляющая новые команды, новые поля в метаданных. Но интерфейс и архитектура остались всё те же, а соответственно всё те же проблемы. Например, установленные через него пакеты нельзя удалить. Теперь именно он используется во всех `setup.py` и точно установлен в каждом вашем окружении.

А ещё когда-то существовал форк `setuptools` под названием `distribute`. Авторы хотели заменить им сам `setuptools`, и даже pip какое-то время рекомендовали использовать именно его, но через 2 года проекты слились в один под именем `setuptools` (https://github.com/pypa/pip/issues/941).

## setup.cfg

Ещё одна интересная попытка починить все проблемы `distutils` и многие другие, о которых я расскажу дальше, была предпринята в проекте [distutils2](https://pypi.org/project/Distutils2/). Его планировали как включить в Python 3.3 под красивым именем `packaging`, так и сделать доступным на pypi в качестве бэкпорта. В итоге идею забросили в пользу `venv`, `setuptools` и `pip`, но задумка была интересная.

Но `distutils2` всё-таки оказал некоторое влияние на то, с чем мы живём сейчас. В 2009 году был написан [PEP-390](https://www.python.org/dev/peps/pep-0390/), описывающий формат `setup.cfg`. Идея была простая: сделать возможность получать метаданные проекта бех его установки или исполнения скриптов. Формат cfg такое себе, потому что он совершенно не гибкий и не стандартизованный, но всё равно, это была отличная идея. PEP в 2013 году отвергли, потому что `distutils2` умер, однако в 2016 году поддержку этого формата [добавили](https://setuptools.readthedocs.io/en/latest/setuptools.html#configuring-setup-using-setup-cfg-files) в `setuptools`.

## sdist

Sdist -- это `tar.gz` архив c кодом пакета и всеми его метаданными. Именно он скачивается с pypi. Формат развивался давно и стихийно, а потому и выглядит крайне занимательно. Внутри лежит директория с названием проекта и его версией, а в этой директории все нужные файлы, такие как README и LICENSE, папки с кодом проекта, иногда тесты, и директория `project_name.egg-info`. Давайте посмотрим, что в ней.

+ `PKG-INFO` -- основные метаданный проекта. Здесь название и версия проекта, описание, homepage и прочее. В общем, почти всё то же самое, что указывается в setup.py. Из интересного ключ `Provides-Extra` который перечисляет, какие у пакета есть extra зависимости. Но самое интересное -- формат файла. Это email сообщение.
+ `requires.txt` -- файл с зависимостями. Кастомный формат, немного напоминает ini. Зависимости описаны примерно как в requirements.txt и разделены на секции. Название секции -- имя extra группы. В начале идут не-extra зависимости, без секции.
+ `entry_point.txt` -- а фот это уже настоящий ini с точками входа в приложение. Entry points позволяют регистрировать консольные команды и плагины для всяких штук типа pytest и flake8.
+ `dependency_links.txt` -- тут указываются ссылки на зависимости, которые нужно забрать не с pypi. Это может быть URL или путь к архиву или скрипту.
+ `SOURCES.txt` -- список всех файлов в архиве.
+ Ну и ещё несколько неинтересных файлов.

В общем, внутри целый зоопарк разных файлов. Все в разных форматах, некоторые пустые, какие-то могут отсутствовать. Sdist -- очень некрасивый и неэффективный формат, и у меня вызывает только грусть.

## wheel

Но не всё так плохо! Sdist в 2012 году решили починить! Появился формат wheel, призванный исправить все проблемы sdist. Он позволил быстрее ставить пакеты, складывать в них скомпилированные C библиотеки, заложил информацию о платформе в название архива. А ещё теперь это zip. Давайте посмотрим, как починили метаданные.

+ `METADATA`. В PEP написано, что это прям то же самое, что и `PKG-INFO`. Верите? Не надо верить.
  + Описание пакета теперь не один из заголовков, а тело сообщения. Вы же помните, что у нас формат -- это email письмо?
  + Появился заголовок `Requires-Dist`, теперь зависимости описаны в нём. Вроде бы, неплохое изменение, но парсить это стало сильно больнее. Всё дело в том, что extra группа для зависимостей, которая рагьше была именем секции, теперь указывается как маркер.
+ `RECORD` -- список всех файлов, как и `SOURCES.txt`, но только теперь тут ещё хэши указаны.

Всё остальное то же самое. Остались `dependency_links.txt` (только теперь файл можно не записывать, если он пустой), остались `entry_points.txt`. Про существование JSON никто так и не вспомнил.

## pypi.org

pypi.org - центральное хранилище Python пакетов, откуда их можно скачать и установить. Он существует с 2003 года и побывал на разных адресах:

+ 2003-2007: python.org/pypi
+ 2007-2017: pypi.python.org
+ 2017-сейчас: pypi.org

Изначально он назывался CheeseShop, сейчас же обрёл более солидное название Warehouse. Ещё довольно примечательно, что с 2006-2007 годах в него всё хотели встроить cheesecake (pycheesecake.org) -- инструмент, для оценки кода по 3 критериям: Installability, Documentation, Code kwalitee (sic!). Был сайт с рейтингом всех пакетов: pypi.pycheesecake.org. Рейтинг качества кода оценивался с помощью PyLint. Он же и занимал первое место в этом рейтинге. Возможно, идею и забросили. Сейчас все ссылки уже мертвы, и я не знаю, что стало с проектом, но internet wayback machine всё помнит.

PyPI практически полностью переписали в 2017 году. Один из моих любимых фактов о новом PyPI, что для загрузки пакетов на него используется эндпоинт `upload.pypi.org/legacy`. А фишка в том, что не legacy эндпоинта нет. Страница `simple` со списком всех проектов на PyPI, которую использует pip, также считается legacy. В общем, когда вам доверили переписать сервис, переименуйте все старые эндпоинты в legacy. Покажите, кто здесь теперь главный.

Как ни странно, именно pypi определяет, как будет выглядеть packaging в питоне. И одним из самых важных факторов здесь я считаю то, что pypi никогда не умел самостоятельно находить метаданные пакета. В самом-самом начале использовалась команда `python setup.py register`, которая отправляла на pypi POST запрос с метаданными пакета, а затме с помощью `python setup.py upload` можно было загружать новые релизы. Всё, что изменилось с того момента -- теперь команда upload так же отправляет и метаданные, а команда register больше не нужна. В результате, pypi просто верит всему, что говорит клиент, загружающий релиз, самостоятельно никакие метаданные не проверяя. И на практике клиент лжёт довольно часто. Поэтому если pypi говорит, что у пакета нет зависимостей, то это значит либо что их действительно нет, либо что пакет был загружен старым клиентом, который не умеет сообщать pypi зависимости пакета.

## pip

Есть в `setuptools`, в числе прочего, команда `easy_install`. Называется она так, потому что команда `install` уже занята для установки текущего пакета: `python setup.py install`. В какой-то момент стало понятно, что устанавливать пакеты становится всё сложнее, и появился `pip`. Он просто был, да и остаётся, в какой-то степени, обёрткой вокруг `setuptools`, которая делает загрузку, установку и удаление пакетов чуть умнее, чем `easy_install`.

Что интересно, в pip была когда-то команда bundle, которая собирала zip-архив с кодом проекта и текстовым файлом со списком зависимостей. Не уверен, чем им не нравился sdist, если он уже тогда существовал. Возможно, хотели стандартизовать, что где лежит, но не дошли даже до того, чтобы метаданные во внутрь складывать. В любом случае, от идеи отказались.

Самое интересное, что нам подарил pip -- формат `requirements.txt` (https://pip.readthedocs.io/en/0.7.2/requirement-format.html). Идея в том, чтобы для простого проекта не заморачиваться с тем, чтобы сделать его пакетом, да и вообще не создавать `setup.py`, а просто перечислить все зависимости в одном теестовом файле. Так как формат изначально задуман исключительно для pip, то это позволило легко добавлять в него фичи, не поддерживаемые setuptools, да и вообще не беспокоиться о читаемости формата кем-то ещё.

Главное, что нужно знать о pip, что там ужасный код. Он настолько плох, что в какой-то момент его полностью перенесли в `pip._internal`, чтобы показать, что pip не может быть использован в качестве библиотеки, и обратная совместимость может быть сломана в любом релизе. Собственно, так и происходит. Я узнаю о каждом новом релизе pip по сломанному CI в `DepHell`. У тому же, запутанность кода делает сложным, а многда даже практически и нереализуемым, добавление новых фич. Поэтому pip вряд ли когда-нибудь научится делать хоть что-нибудь, кроме установки зависимостей из собранных пакетов, `setup.py` или `requirements.txt`. Он даже не использует новое JSON API PyPI.

## pip-tools

Вот есть у нас проект на питоне, мы его локально протестировали и знаем, что в нашем окружении всё работает. Теперь нужно отправить всё на прод, чтобы все зависимости были тех же версий, что и у нас локально, потому что с другими версиями что-то может и не работать. Соответственно, надо сделать ещё один `requirements.txt`, в котором будут указаны вообще все зависимости в окружении, а не только те, с котороми проект работает напрямую, и для каждого указана конкретная версия. Это и называется lock-файлом. Просто Docker контейнер для этих задач не подходит, потому что мы хотим видеть все вресии как код и обновлять их при необходимости как код, причём иногда только отдельные зависимости, а не все сразу.

В pip для этого есть команда `pip freeze`. Она создаёт файл зависимостей со всеми пакетами и их версиями, установленными в текущем окружении. Отсюда следуют ограничения: обязательно нужно чистое виртуальное окружение, а если хочется разделять dev и prod зависимости, то и окружения для этого нужны раздельные. Ну и решением этих проблем стал инструмент `pip-tools`. `pip-tools` имеет внутри всего две команды: для создания lock-файла, и для синхронизации окружения с lock-файлом. Делает он это посылая запросы на PyPI с помощью pip, выкачивая релизы и вытаскивая из них зависимости.

## pipenv

История Pipfile начинается в обсуждении в репозитории pip под названием "Requirements 2.0" (https://github.com/pypa/pip/issues/1795). Совсем скоро это переехало в pypa/pipfile. Мотивация -- сделать удобную альтернативу requirements.txt, которую легко читать людям и машинам, легко выучить, легко подсветить синтаксис. Pipfile -- файл, описывающий в формате toml все зависимости проекта, python версию, кастомные репозитории. Всё, метаданных нет, так что для пакетов формат использовать нельзя. Ещё есть Pipfile.lock, это json с полным списком зависимостей в окружении проекта. В том же репозитории лежит библиотека, которая должна была стать стандартным парсером для формата.

И вот Pipfile, созданный для pip 4 года назад, до сих пор pip не поддерживается. Одна из причин -- в pip сложно что-то добавить, мы уже говорили об этом. И вот появился pipenv, в который решили добавить не только поддержку Pipfile, но и работу с виртуальными окружениями, и ещё какие-то мелкие фичи. Также проект задаёт свои требования: Pipfile на проект может быть только один, и в нём может быть только 2 окружения -- основное и dev. Кто не согласен, авторам плевать. Довольно быстро pipenv также вырос во что-то страшное, с кучей зависимостей, проблем и плохого кода. Начали это немного приводить в порядок, появилась организация sarugaku, которая через такие же громады нечитаемого и переусложнённого кода решается решать проблемы pipenv, чтобы потом эти пакеты в самом pipenv и использовать. Всё, что нужно знать обо всём этом, уже сокрыто в названии:

> Саругаку (яп. 猿楽 саругаку, «обезьянья музыка») — вид японского народного фарса, содержащего акробатику, клоунаду, жонглирование, хождение на ходулях, танцы, фокусы и т.д.

+ https://discuss.python.org/t/structured-exchangeable-lock-file-format-requirements-txt-2-0/876
+ https://github.com/pypa/pipfile/issues/108

## poetry

## flit


## Resolving

Когда мы говорим про lock-файлы, на сцену выходит задача резолвинга. Дело в том, что довольно часто определенные пакеты совместимы не со всеми версиями своих зависимостей. Иногда им нужны фичи, которые появились только в определенной версии зависимости, иногда они наоборот, не поддерживают более новые версии зависимости, потому что там произошли какие-то несовместимые изменения. Всё это должно быть отражено в constraint'ах пакета, причем эти constraint'ы могут изменяться от релиза к релизу. И особенность питона в том, что мы не можем предоставить каждому пакету свои собственные версии его зависимостей. Нет, нам нужно для каждого пакета выбрать одну наилучшую версию, которая удовлетворит constraint'ам всех пакетов в окружении, самого проекта, да ещё, желательно, будет не слишком старая.

Это типичная `P!=NP` задача. Проверить, что мы собрали окружение правильно, довольно просто, проверив все constraint'ы. А вот найти такое окружение можно, по идее, только перебрав все возможные варианты. Секрет успеха состоит в том, чтобы хоть как-то сократить число возможных вариантов, а также перебрать в первую очередь только наиболее вероятные. Ни один инструмент не будет перебирать все варианты и остановится после какого-то количества итераций. Главное, что нужно знать про проблему `P!=NP` -- мы никогда не можем сказать для такой задачи, что наш алгоритм самый оптимальный. Маловероятно, что будет доказано равенство P и NP, но если будет, то это значит, что мы сможем просто сразу же взять правильное окружение, уделав все существующие решения.

Наихудший резолвер в pip, и это древняя и хорошо изветсная проблема. Резолвер в pip работает по принципу "first round wins". То есть сначала выберутся совместимые версии зависимостей самого проекта, потом зависимости зависимостей и так далее. Если же на каком-то этапе у нас появляется пакет, которому нужен пакет, который был зарезолвен на предудщих этапах, но его версия не совместима с тем, что нужно этому пакету -- ну, не судьба. pip выведет предупреждение и продолжит работать, как будто ничего не произошло. Есть шансы, что это будет работать, но особо надеяться на это не стоит. Избежать резолвера можно делегировав резолвинг кому-нибудь ещё. Если pip сказать конкрентные версии всего, что нужно установить, проблем не возникнет.

Pip-tools старается чуть больше. Он также идёт по всем зависимостям раунд за раундом, но когда в новом раунде пакет зависит от каого-то пакета из предыдущего раунда, этот пакет попадёт в новый раунд, где он снова попытается разрезолвиться, чтобы удовлетворить всем. Проблема в том, что старые зависимости этого пакета остаются в графе. То есть, если у пакета между тем релизом, что был выбран в началае, и тем, что выбран позднее, были отличия в зависимостях, то эти самые лишние зависимости никуда не денутся. Я не увидел, чтобы pip-tools их после всего удалял из графа, но даже если он это и делает, то они всё равно остаются в процессе резолвинга, что может сделать резолвинг невозможным, тогда как на самом деле всё возможно.

Про pipenv сказать можно не так уж и много. Он внутри использует pip-tools. Они как-то стараются улучшать свой резолвер, накручивать что-то вокруг, но становится только хуже: медленнее, больше грязного кода, и вот уже кто-либо перестаёт понимать, что вообще происходит. И если pip-tools умеет брать в расчёт старый lock-файл, то pipenv каждый раз начинает всё сначала. Как результат проблемы с резолвингом в какой-то момент неизбежно возникают. Именно из-за этих проблем я и начал делать DepHell: хотелось возможность применить умный резолвинг туда, где его нет.

В poetry всё гораздо лучше. Это первый инструмент в мире python с умным резолвингом. В нём реализован алгоритм PubGrub используемый в Dart, и он работает прям хорошо. В Poetry всё очень странно и больно с выбором совместимой версии Python, но это уже особенности самого poetry, а не алгоритма. Я очень старался разобраться в этом алгоритме, но по нему есть всего одна статья от автора алгоритма, которая больше маркетинговая, делает много допущений, и вообще особо не несёт какой-то смысловой нагрузки. Код на Dart я читать не умею, а исходники Poetry больше поэзия, чем код. Например, модули там внутри называются masonry, puzzle, mixology, и я о сих пор не особо представляю, что это всё значит.

## DepHell
